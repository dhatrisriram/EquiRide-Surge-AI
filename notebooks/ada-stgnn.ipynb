{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13716839,"sourceType":"datasetVersion","datasetId":8726530}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ===========================================================\n# ONE-CELL FULL INSTALL: PyTorch 2.5.1 + PyG 2.6.1 (Kaggle GPU)\n# ===========================================================\n\nprint(\"Installing PyTorch 2.5.1 + cu121 ...\")\n!pip install -q torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 \\\n    --index-url https://download.pytorch.org/whl/cu121\n\nprint(\"Installing PyTorch Geometric 2.6.1 ...\")\n!pip install -q pyg-lib torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric \\\n    -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n\nprint(\"Testing imports...\")\nimport torch\nimport torch_geometric\n\nprint(\"\\n====================================\")\nprint(\"Torch version:\", torch.__version__)\nprint(\"PyG version:\", torch_geometric.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"====================================\")\nprint(\"INSTALLATION COMPLETE. DO NOT RESTART.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T13:45:29.506185Z","iopub.execute_input":"2025-11-13T13:45:29.506898Z","iopub.status.idle":"2025-11-13T13:46:52.359195Z","shell.execute_reply.started":"2025-11-13T13:45:29.506871Z","shell.execute_reply":"2025-11-13T13:46:52.358321Z"}},"outputs":[{"name":"stdout","text":"Installing PyTorch 2.5.1 + cu121 ...\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling PyTorch Geometric 2.6.1 ...\nTesting imports...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/libpyg.so: undefined symbol: _ZN3c1010Dispatcher17runRecordFunctionERN2at14RecordFunctionESt17reference_wrapperIKNS_14FunctionSchemaEENS_11DispatchKeyE\n  import torch_geometric.typing\n/usr/local/lib/python3.11/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n  import torch_geometric.typing\n/usr/local/lib/python3.11/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n  import torch_geometric.typing\n/usr/local/lib/python3.11/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n  import torch_geometric.typing\n/usr/local/lib/python3.11/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n  import torch_geometric.typing\n","output_type":"stream"},{"name":"stdout","text":"\n====================================\nTorch version: 2.5.1+cu121\nPyG version: 2.7.0\nCUDA available: True\n====================================\nINSTALLATION COMPLETE. DO NOT RESTART.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# === CELL 1: CONFIG, PATHS, DEPENDENCY SETUP ===\nINPATH = '/kaggle/input/ada-gnn2/'\nOUTPATH = '/kaggle/working/'\nTIME_BIN = '15T'\nK_NEIGHBORS = 6\nL = 4                 # history length (settable)\nHORIZON = 1\nTMP_H = 64            # temporal conv output channels\nGCN_H = 128           # graph conv hidden size\nBATCHING = False      # per-time full-graph training: keep False (simpler)\nDEVICE = 'cuda' if __import__('torch').cuda.is_available() else 'cpu'\nprint(\"INPATH:\", INPATH, \"OUTPATH:\", OUTPATH, \"DEVICE:\", DEVICE)\n# Try imports\nimport os, math, sys, time\nimport numpy as np, pandas as pd\nimport torch\ntry:\n    import h3\nexcept Exception as e:\n    print(\"h3 import failed:\", e)\ntry:\n    import torch_geometric\n    from torch_geometric.data import Data\n    from torch_geometric.nn import SAGEConv\nexcept Exception as e:\n    print(\"torch_geometric import error (best-effort):\", e)\n    # We will try to continue; Kaggle often already has PyG installed.\nprint(\"Base libraries imported.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T13:46:52.360633Z","iopub.execute_input":"2025-11-13T13:46:52.361044Z","iopub.status.idle":"2025-11-13T13:46:52.368494Z","shell.execute_reply.started":"2025-11-13T13:46:52.361021Z","shell.execute_reply":"2025-11-13T13:46:52.367298Z"}},"outputs":[{"name":"stdout","text":"INPATH: /kaggle/input/ada-gnn2/ OUTPATH: /kaggle/working/ DEVICE: cuda\nh3 import failed: No module named 'h3'\nBase libraries imported.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# === CELL 2: UTILITIES (robust datetime parser, zone repair) ===\nfrom dateutil import parser as dparser\nimport re\n\ndef robust_parse_datetime(s):\n    if pd.isna(s): return pd.NaT\n    s = str(s).strip().replace(\"\\uFEFF\",\"\").replace(\"\\xa0\",\" \").strip('\"').strip(\"'\")\n    fmts = [\"%d-%m-%Y %H:%M\",\"%d-%m-%Y %H:%M:%S\",\"%Y-%m-%d %H:%M\",\"%Y-%m-%d %H:%M:%S\"]\n    for f in fmts:\n        try:\n            return pd.to_datetime(s, format=f)\n        except:\n            pass\n    try:\n        return pd.to_datetime(s, dayfirst=True)\n    except:\n        pass\n    try:\n        return dparser.parse(s, dayfirst=True)\n    except:\n        return pd.NaT\n\ndef floor_to_bin(ts, freq=TIME_BIN):\n    if pd.isna(ts): return pd.NaT\n    return pd.to_datetime(ts).floor(freq)\n\ndef repair_zone_token(tok):\n    if tok is None: return tok\n    s = str(tok).strip()\n    if s.endswith(\".0\"): s = s[:-2]\n    s = s.strip('\"').strip(\"'\")\n    return s\n\ndef is_valid_h3(h):\n    if h is None: return False\n    s = str(h).strip().lower()\n    if not re.fullmatch(r'[0-9a-f]+', s): return False\n    try:\n        h3.h3_to_geo(s); return True\n    except: return False\n\nprint(\"Utils ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T13:46:52.369258Z","iopub.execute_input":"2025-11-13T13:46:52.369438Z","iopub.status.idle":"2025-11-13T13:46:52.387813Z","shell.execute_reply.started":"2025-11-13T13:46:52.369423Z","shell.execute_reply":"2025-11-13T13:46:52.387167Z"}},"outputs":[{"name":"stdout","text":"Utils ready.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# === CELL 3: READ CSVs (robustly, all-as-string initial read) ===\nagg_path = os.path.join(INPATH, \"aggregated_zone_data.csv\")\nlgb_path = os.path.join(INPATH, \"forecast_15min_predictions.csv\")\nraw_path = os.path.join(INPATH, \"Data_set.csv\")\n\nif not os.path.exists(agg_path):\n    raise FileNotFoundError(f\"aggregated_zone_data.csv not found in {agg_path}\")\nagg_df = pd.read_csv(agg_path, dtype=str, low_memory=False)\nprint(\"Loaded aggregated:\", agg_df.shape)\nlgb_df = None\nif os.path.exists(lgb_path):\n    lgb_df = pd.read_csv(lgb_path, dtype=str, low_memory=False)\n    print(\"Loaded LGB preds:\", lgb_df.shape)\nelse:\n    print(\"No LGB preds found.\")\nraw_df = None\nif os.path.exists(raw_path):\n    try:\n        raw_df = pd.read_csv(raw_path, dtype=str, low_memory=False)\n        print(\"Loaded raw Data_set:\", raw_df.shape)\n    except Exception as e:\n        print(\"raw read failed:\", e)\ndisplay(agg_df.head(3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T13:46:52.388509Z","iopub.execute_input":"2025-11-13T13:46:52.388705Z","iopub.status.idle":"2025-11-13T13:46:52.933064Z","shell.execute_reply.started":"2025-11-13T13:46:52.388680Z","shell.execute_reply":"2025-11-13T13:46:52.932474Z"}},"outputs":[{"name":"stdout","text":"Loaded aggregated: (17332, 12)\nLoaded LGB preds: (67, 3)\nLoaded raw Data_set: (18000, 41)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"              zone             Datetime Completed Trips     Traffic Volume  \\\n0  8986d880d77ffff  2024-01-05 18:30:00            85.0              481.0   \n1  8986d880d77ffff  2024-01-07 18:45:00           241.0  1324.398950135941   \n2  8986d880d77ffff  2024-01-09 18:30:00            91.0              466.0   \n\n  Average Speed   Congestion Level Temperature Drivers' Earnings  \\\n0        47.121             16.321      23.504          3519.747   \n1          32.5  42.12779439509311        30.3          85825.04   \n2         47.54             16.596      23.281          2065.448   \n\n  Distance Travelled (km) event importance           Bookings event type  \n0                 513.369              0.0              112.0   No Event  \n1                 1954.51             0.87  526.1780508958601      Local  \n2                 502.273            0.049              106.0   No Event  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>zone</th>\n      <th>Datetime</th>\n      <th>Completed Trips</th>\n      <th>Traffic Volume</th>\n      <th>Average Speed</th>\n      <th>Congestion Level</th>\n      <th>Temperature</th>\n      <th>Drivers' Earnings</th>\n      <th>Distance Travelled (km)</th>\n      <th>event importance</th>\n      <th>Bookings</th>\n      <th>event type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8986d880d77ffff</td>\n      <td>2024-01-05 18:30:00</td>\n      <td>85.0</td>\n      <td>481.0</td>\n      <td>47.121</td>\n      <td>16.321</td>\n      <td>23.504</td>\n      <td>3519.747</td>\n      <td>513.369</td>\n      <td>0.0</td>\n      <td>112.0</td>\n      <td>No Event</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8986d880d77ffff</td>\n      <td>2024-01-07 18:45:00</td>\n      <td>241.0</td>\n      <td>1324.398950135941</td>\n      <td>32.5</td>\n      <td>42.12779439509311</td>\n      <td>30.3</td>\n      <td>85825.04</td>\n      <td>1954.51</td>\n      <td>0.87</td>\n      <td>526.1780508958601</td>\n      <td>Local</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8986d880d77ffff</td>\n      <td>2024-01-09 18:30:00</td>\n      <td>91.0</td>\n      <td>466.0</td>\n      <td>47.54</td>\n      <td>16.596</td>\n      <td>23.281</td>\n      <td>2065.448</td>\n      <td>502.273</td>\n      <td>0.049</td>\n      <td>106.0</td>\n      <td>No Event</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# === CELL 4: CLEAN DATETIME and ZONE column ===\n# Detect Datetime column\ndatetime_cols = [c for c in agg_df.columns if \"date\" in c.lower() or \"time\" in c.lower()]\ndt_col = \"Datetime\" if \"Datetime\" in agg_df.columns else (datetime_cols[0] if datetime_cols else None)\nif dt_col is None:\n    raise ValueError(\"No datetime-like column found\")\nagg_df[\"Datetime_raw\"] = agg_df[dt_col].astype(str)\nagg_df[\"Datetime_parsed\"] = agg_df[\"Datetime_raw\"].apply(robust_parse_datetime)\nif agg_df[\"Datetime_parsed\"].notna().sum() == 0:\n    print(\"First 50 datetime strings:\", agg_df[\"Datetime_raw\"].head(50).tolist())\n    raise ValueError(\"Datetime parsing failed\")\nagg_df[\"Datetime\"] = agg_df[\"Datetime_parsed\"].apply(lambda x: floor_to_bin(x, TIME_BIN))\n# Zone column\nzone_candidates = [c for c in agg_df.columns if c.lower() in (\"zone\",\"h3_index\",\"area\",\"zone_id\",\"h3\")]\nzone_col = zone_candidates[0] if zone_candidates else agg_df.columns[0]\nagg_df[zone_col] = agg_df[zone_col].astype(str).apply(repair_zone_token)\nagg_df = agg_df.dropna(subset=[zone_col, \"Datetime\"])\nagg_df[zone_col] = agg_df[zone_col].astype(str)\nprint(\"Using zone column:\", zone_col, \"rows after drop:\", len(agg_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T13:46:52.934798Z","iopub.execute_input":"2025-11-13T13:46:52.935014Z","iopub.status.idle":"2025-11-13T13:46:57.130186Z","shell.execute_reply.started":"2025-11-13T13:46:52.934998Z","shell.execute_reply":"2025-11-13T13:46:57.129504Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_122/1714559376.py:25: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n  return pd.to_datetime(ts).floor(freq)\n","output_type":"stream"},{"name":"stdout","text":"Using zone column: zone rows after drop: 17332\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# === CELL 5: FEATURE DETECTION & TARGET SETUP ===\n# Normalize column names and detect target\nagg_df.columns = [c.strip() for c in agg_df.columns]\ntarget_col = None\nfor cand in [\"Bookings\",\"bookings\",\"Completed Trips\",\"completed trips\"]:\n    if cand in agg_df.columns:\n        target_col = cand; break\nif target_col is None:\n    for c in agg_df.columns:\n        if \"book\" in c.lower() or \"trip\" in c.lower():\n            target_col = c; break\nif target_col is None:\n    raise ValueError(\"Could not detect Booking target column.\")\n# Convert target numeric\nagg_df[target_col] = pd.to_numeric(agg_df[target_col].astype(str).str.replace(\",\",\"\"), errors='coerce')\n# Auto-detect numeric features (exclude id columns)\nexclude = {zone_col, \"Datetime\", \"Datetime_raw\", \"Datetime_parsed\", target_col}\nfeature_cols = []\nfor c in agg_df.columns:\n    if c in exclude: continue\n    try:\n        agg_df[c] = pd.to_numeric(agg_df[c].astype(str).str.replace(\",\",\"\").replace(\" \", \"\"), errors='coerce')\n    except:\n        pass\n    if pd.api.types.is_numeric_dtype(agg_df[c]):\n        feature_cols.append(c)\nprint(\"Target:\", target_col)\nprint(\"Detected features:\", feature_cols)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T13:46:57.130938Z","iopub.execute_input":"2025-11-13T13:46:57.131187Z","iopub.status.idle":"2025-11-13T13:46:57.238016Z","shell.execute_reply.started":"2025-11-13T13:46:57.131169Z","shell.execute_reply":"2025-11-13T13:46:57.237414Z"}},"outputs":[{"name":"stdout","text":"Target: Bookings\nDetected features: ['Completed Trips', 'Traffic Volume', 'Average Speed', 'Congestion Level', 'Temperature', \"Drivers' Earnings\", 'Distance Travelled (km)', 'event importance', 'event type']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# === CELL 6: MERGE LGB PREDICTIONS SAFELY (optional) ===\nif lgb_df is None:\n    print(\"No LGB file to merge.\")\nelse:\n    lgb = lgb_df.copy()\n    # detect zone/time/pred columns\n    l_time = [c for c in lgb.columns if \"time\" in c.lower() or \"next\" in c.lower()]\n    l_zone = [c for c in lgb.columns if \"zone\" in c.lower() or \"h3\" in c.lower()]\n    l_pred = [c for c in lgb.columns if \"pred\" in c.lower() and \"book\" in c.lower()]\n    if not l_time or not l_zone:\n        print(\"Could not detect LGB columns; skipping merge.\")\n    else:\n        time_col = l_time[0]\n        zone_col_lgb = l_zone[0]\n        pred_col = l_pred[0] if l_pred else lgb.columns[-1]\n        lgb[\"Datetime\"] = lgb[time_col].apply(robust_parse_datetime).apply(lambda x: floor_to_bin(x, TIME_BIN))\n        lgb[zone_col_lgb] = lgb[zone_col_lgb].astype(str)\n        lgb[\"pred_bookings\"] = pd.to_numeric(lgb[pred_col].astype(str).str.replace(\",\",\"\"), errors='coerce')\n        merge_src = lgb[[zone_col_lgb, \"Datetime\", \"pred_bookings\"]].rename(columns={zone_col_lgb:zone_col})\n        agg_df = agg_df.merge(merge_src, on=[zone_col, \"Datetime\"], how=\"left\")\n        if \"pred_bookings\" in agg_df.columns and \"pred_bookings\" not in feature_cols:\n            feature_cols.append(\"pred_bookings\")\n        print(\"Merged LGB preds. New feature count:\", len(feature_cols))\n# Sort and impute simple missing values in features (per-zone)\nagg_df = agg_df.sort_values([zone_col, \"Datetime\"])\nexisting_feature_cols = [c for c in feature_cols if c in agg_df.columns]\nfor c in existing_feature_cols:\n    agg_df[c] = agg_df.groupby(zone_col)[c].transform(lambda x: x.interpolate().bfill().ffill())\n# impute target\nagg_df[target_col] = agg_df.groupby(zone_col)[target_col].transform(lambda x: x.interpolate().bfill().ffill())\nprint(\"After imputation, any NaN in target?\", agg_df[target_col].isna().any())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T13:46:57.238670Z","iopub.execute_input":"2025-11-13T13:46:57.238871Z","iopub.status.idle":"2025-11-13T13:46:57.585624Z","shell.execute_reply.started":"2025-11-13T13:46:57.238855Z","shell.execute_reply":"2025-11-13T13:46:57.585034Z"}},"outputs":[{"name":"stdout","text":"Merged LGB preds. New feature count: 10\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_122/1714559376.py:25: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n  return pd.to_datetime(ts).floor(freq)\n","output_type":"stream"},{"name":"stdout","text":"After imputation, any NaN in target? False\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# === CELL 7: BUILD KNN ADJACENCY FROM H3 CENTROIDS (robust) ===\nfrom sklearn.neighbors import NearestNeighbors\nimport csv\n\n# build candidate zones from agg_df (unique tokens)\ncandidate_zones = sorted(agg_df[zone_col].unique())\nprint(\"Candidate zones:\", len(candidate_zones))\nzone_centroids = {}\ninvalid = []\nfor z in candidate_zones:\n    zt = str(z).strip().lower()\n    if is_valid_h3(zt):\n        try:\n            lat, lon = h3.h3_to_geo(zt)\n            zone_centroids[zt] = (lat, lon)\n        except:\n            invalid.append(zt)\n    else:\n        invalid.append(zt)\nprint(\"Valid centroids:\", len(zone_centroids), \"Invalid tokens:\", len(invalid))\n# fallback to raw_df lat/lon if available for invalid tokens\nif len(zone_centroids) < 2 and raw_df is not None:\n    print(\"Attempt fallback from raw_df\")\n    possible_zone_cols = [c for c in raw_df.columns if \"h3\" in c.lower() or \"zone\" in c.lower()]\n    lat_cols = [c for c in raw_df.columns if \"lat\" in c.lower()]\n    lon_cols = [c for c in raw_df.columns if \"lon\" in c.lower() or \"lng\" in c.lower()]\n    if possible_zone_cols and lat_cols and lon_cols:\n        rz = possible_zone_cols[0]; latc = lat_cols[0]; lonc = lon_cols[0]\n        for token, grp in raw_df.groupby(rz):\n            tok = str(token).strip()\n            if tok in zone_centroids: continue\n            try:\n                lat = pd.to_numeric(grp[latc], errors='coerce').mean()\n                lon = pd.to_numeric(grp[lonc], errors='coerce').mean()\n                if not np.isnan(lat) and not np.isnan(lon):\n                    zone_centroids[tok] = (lat, lon)\n            except:\n                pass\nprint(\"Total centroids:\", len(zone_centroids))\nif len(zone_centroids) < 2:\n    raise ValueError(\"Not enough centroids to build adjacency.\")\n# zone_list (order) & coords\nzone_list = sorted(zone_centroids.keys())\ncoords = np.array([zone_centroids[z] for z in zone_list])\n# knn\nk = min(K_NEIGHBORS, max(1, len(zone_list)-1))\nn_neighbors = min(k+1, len(zone_list))\nnn = NearestNeighbors(n_neighbors=n_neighbors)\nnn.fit(coords)\ndists, idxs = nn.kneighbors(coords)\nedges = set()\nfor i, nbrs in enumerate(idxs):\n    for j in nbrs[1:]:\n        edges.add((i,j)); edges.add((j,i))\nedges_list = sorted(edges)\n# save edges csv mapping indices -> zone tokens\nwith open(os.path.join(OUTPATH, \"graph_edges.csv\"), \"w\", newline=\"\") as f:\n    w = csv.writer(f); w.writerow([\"src_idx\",\"dst_idx\",\"src_zone\",\"dst_zone\"])\n    for a,b in edges_list:\n        w.writerow([a,b,zone_list[a],zone_list[b]])\nprint(\"Saved graph_edges.csv with\", len(edges_list), \"edges to\", OUTPATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T13:46:57.586380Z","iopub.execute_input":"2025-11-13T13:46:57.586622Z","iopub.status.idle":"2025-11-13T13:46:58.517579Z","shell.execute_reply.started":"2025-11-13T13:46:57.586603Z","shell.execute_reply":"2025-11-13T13:46:58.516709Z"}},"outputs":[{"name":"stdout","text":"Candidate zones: 67\nValid centroids: 0 Invalid tokens: 67\nAttempt fallback from raw_df\nTotal centroids: 67\nSaved graph_edges.csv with 504 edges to /kaggle/working/\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# === CELL 8: BUILD DENSE TENSORS (fast vectorized) X_tensor, Y_tensor ===\n# Build timeline\nall_times = pd.date_range(start=agg_df[\"Datetime\"].min(), end=agg_df[\"Datetime\"].max(), freq=TIME_BIN)\nT = len(all_times); Z = len(zone_list); F = len(feature_cols)\nprint(\"Time bins:\", T, \"Zones:\", Z, \"Features:\", F)\nzone_to_idx = {z:i for i,z in enumerate(zone_list)}\ntime_to_idx = {t:i for i,t in enumerate(all_times)}\n# prepare arrays\nX_tensor = np.full((T, Z, F), np.nan, dtype=np.float32)\nY_tensor = np.full((T, Z), np.nan, dtype=np.float32)\n# map feature columns in agg_df to feature_cols order; create copy of values as float\nfor _, row in agg_df.iterrows():\n    z = str(row[zone_col]).strip().lower()\n    dt = row[\"Datetime\"]\n    if z in zone_to_idx and pd.notna(dt):\n        ti = time_to_idx.get(pd.Timestamp(dt), None)\n        if ti is None: continue\n        zi = zone_to_idx[z]\n        vals = []\n        for c in feature_cols:\n            try:\n                v = float(row[c]) if pd.notna(row[c]) else np.nan\n            except:\n                v = np.nan\n            vals.append(v)\n        X_tensor[ti, zi, :] = np.array(vals, dtype=np.float32)\n        Y_tensor[ti, zi] = float(row[target_col]) if pd.notna(row[target_col]) else np.nan\nprint(\"Observed filled cells (feature0):\", np.sum(~np.isnan(X_tensor[:,:,0])))\n# Impute per-zone along time axis\nfor zi in range(Z):\n    for fi in range(F):\n        col = X_tensor[:, zi, fi]\n        mask = ~np.isnan(col)\n        if mask.sum() == 0:\n            X_tensor[:, zi, fi] = 0.0\n        elif mask.sum() == 1:\n            X_tensor[:, zi, fi] = col[mask][0]\n        else:\n            idxs = np.where(mask)[0]; vals = col[mask]\n            X_tensor[:, zi, fi] = np.interp(np.arange(T), idxs, vals)\n    # target\n    coly = Y_tensor[:, zi]; masky = ~np.isnan(coly)\n    if masky.sum() == 0:\n        Y_tensor[:, zi] = 0.0\n    elif masky.sum() == 1:\n        Y_tensor[:, zi] = coly[masky][0]\n    else:\n        idxs = np.where(masky)[0]; vals = coly[masky]\n        Y_tensor[:, zi] = np.interp(np.arange(T), idxs, vals)\nprint(\"Imputation done. Any NaNs left?\", np.isnan(X_tensor).any(), np.isnan(Y_tensor).any())\n# quick save (optional)\nnp.save(os.path.join(OUTPATH,\"X_tensor.npy\"), X_tensor)\nnp.save(os.path.join(OUTPATH,\"Y_tensor.npy\"), Y_tensor)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T13:46:58.518503Z","iopub.execute_input":"2025-11-13T13:46:58.519170Z","iopub.status.idle":"2025-11-13T13:47:01.558503Z","shell.execute_reply.started":"2025-11-13T13:46:58.519147Z","shell.execute_reply":"2025-11-13T13:47:01.557575Z"}},"outputs":[{"name":"stdout","text":"Time bins: 70075 Zones: 67 Features: 10\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_122/2138261527.py:3: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n  all_times = pd.date_range(start=agg_df[\"Datetime\"].min(), end=agg_df[\"Datetime\"].max(), freq=TIME_BIN)\n","output_type":"stream"},{"name":"stdout","text":"Observed filled cells (feature0): 17332\nImputation done. Any NaNs left? False False\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# === CELL 9: Build ST-GCN sliding sequences (N, Z, F, L) and splits ===\n# sequences: for t in [L-1 .. T-1) we predict t+1\nseq_X = []\nseq_Y = []\ntime_from_list = []\ntime_to_list = []\nfor t in range(L-1, T-1):\n    Xw = X_tensor[t-(L-1):t+1, :, :]    # (L, Z, F)\n    Xw_tf = np.transpose(Xw, (1,2,0)).astype(np.float32)  # (Z, F, L)\n    yw = Y_tensor[t+1, :].astype(np.float32)              # (Z,)\n    seq_X.append(Xw_tf); seq_Y.append(yw)\n    time_from_list.append(all_times[t]); time_to_list.append(all_times[t+1])\nseq_X = np.stack(seq_X, axis=0); seq_Y = np.stack(seq_Y, axis=0)\nN = seq_X.shape[0]\nprint(\"Built seqs N:\", N, \"seq_X shape:\", seq_X.shape, \"seq_Y shape:\", seq_Y.shape)\n# temporal split\nn_train = int(0.7 * N); n_val = int(0.15 * N)\ntrain_idx = np.arange(0, n_train); val_idx = np.arange(n_train, n_train+n_val); test_idx = np.arange(n_train+n_val, N)\nprint(\"Train/Val/Test sizes:\", len(train_idx), len(val_idx), len(test_idx))\n# Save small metadata for checks\nnp.save(os.path.join(OUTPATH,\"zone_list.npy\"), np.array(zone_list))\nnp.save(os.path.join(OUTPATH,\"feature_cols.npy\"), np.array(feature_cols))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T13:47:01.559389Z","iopub.execute_input":"2025-11-13T13:47:01.559703Z","iopub.status.idle":"2025-11-13T13:47:03.734100Z","shell.execute_reply.started":"2025-11-13T13:47:01.559677Z","shell.execute_reply":"2025-11-13T13:47:03.733346Z"}},"outputs":[{"name":"stdout","text":"Built seqs N: 70071 seq_X shape: (70071, 67, 10, 4) seq_Y shape: (70071, 67)\nTrain/Val/Test sizes: 49049 10510 10512\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# === CELL 10: Build PyG Data list (seq in data.seq) ===\nimport torch\nfrom torch_geometric.data import Data\nedge_index = torch.tensor(edges_list, dtype=torch.long).t().contiguous()\ndef build_data_from_seq(x_np, y_np):\n    data = Data()\n    data.seq = torch.from_numpy(x_np).float()   # (Z, F, L)\n    data.y = torch.from_numpy(y_np).float().unsqueeze(1)  # (Z,1)\n    data.edge_index = edge_index\n    data.mask = torch.ones(x_np.shape[0], dtype=torch.bool)\n    return data\n\ntrain_data = [build_data_from_seq(seq_X[i], seq_Y[i]) for i in train_idx]\nval_data = [build_data_from_seq(seq_X[i], seq_Y[i]) for i in val_idx]\ntest_data = [build_data_from_seq(seq_X[i], seq_Y[i]) for i in test_idx]\nprint(\"Data counts -> train:\", len(train_data), \"val:\", len(val_data), \"test:\", len(test_data))\nprint(\"Example seq shape:\", train_data[0].seq.shape, \"y shape:\", train_data[0].y.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T13:47:03.734895Z","iopub.execute_input":"2025-11-13T13:47:03.735264Z","iopub.status.idle":"2025-11-13T13:47:06.554901Z","shell.execute_reply.started":"2025-11-13T13:47:03.735245Z","shell.execute_reply":"2025-11-13T13:47:06.554089Z"}},"outputs":[{"name":"stdout","text":"Data counts -> train: 49049 val: 10510 test: 10512\nExample seq shape: torch.Size([67, 10, 4]) y shape: torch.Size([67, 1])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# === CELL 11: ST-GCN model definition (temporal conv + SAGE graph conv) ===\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv\n\nF_count = len(feature_cols)\nIN_CH = F_count\nIN_L = L\nTMP_H = TMP_H\nGCN_H = GCN_H\n\nclass STGCN_simple(nn.Module):\n    def __init__(self, in_ch, L_steps, tmp_out=64, gcn_hidden=128, num_gcn_layers=2, dropout=0.2):\n        super().__init__()\n        self.temp_conv = nn.Conv1d(in_ch, tmp_out, kernel_size=3, padding=1)  # input channels = features, conv along L\n        self.gconvs = nn.ModuleList([SAGEConv(tmp_out, gcn_hidden)] + [SAGEConv(gcn_hidden, gcn_hidden) for _ in range(num_gcn_layers-1)])\n        self.head = nn.Linear(gcn_hidden, 1)\n        self.dropout = dropout\n    def forward(self, data):\n        # data.seq: (Z, F, L)\n        seq = data.seq.to(next(self.parameters()).device)    # (Z, F, L)\n        x = self.temp_conv(seq)    # (Z, tmp_out, L)\n        x = F.relu(x)\n        x = x.mean(dim=2)          # (Z, tmp_out)\n        edge_index = data.edge_index.to(next(self.parameters()).device)\n        for conv in self.gconvs:\n            x = conv(x, edge_index)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        out = self.head(x)   # (Z,1)\n        return out\n\nmodel = STGCN_simple(in_ch=IN_CH, L_steps=IN_L, tmp_out=TMP_H, gcn_hidden=GCN_H).to(DEVICE)\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T13:47:06.555754Z","iopub.execute_input":"2025-11-13T13:47:06.556083Z","iopub.status.idle":"2025-11-13T13:47:06.779246Z","shell.execute_reply.started":"2025-11-13T13:47:06.556065Z","shell.execute_reply":"2025-11-13T13:47:06.778387Z"}},"outputs":[{"name":"stdout","text":"STGCN_simple(\n  (temp_conv): Conv1d(10, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n  (gconvs): ModuleList(\n    (0): SAGEConv(64, 128, aggr=mean)\n    (1): SAGEConv(128, 128, aggr=mean)\n  )\n  (head): Linear(in_features=128, out_features=1, bias=True)\n)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# === CELL 12: TRAIN ST-GCN (early stopping on val MAE) ===\nimport torch.optim as optim\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport numpy as np, time\n\noptimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\ncriterion = torch.nn.MSELoss()\nbest_val_mae = float('inf'); best_state = None\npatience = 8; patience_ct = 0\nEPOCHS = 20\n\ndef eval_list(model, dlist):\n    model.eval()\n    preds, trues = [], []\n    with torch.no_grad():\n        for d in dlist:\n            d = d.to(DEVICE)\n            out = model(d).squeeze(1).cpu().numpy()\n            true = d.y.squeeze(1).cpu().numpy()\n            mask = d.mask.cpu().numpy()\n            preds.extend(out[mask].tolist()); trues.extend(true[mask].tolist())\n    if len(trues)==0: return {'mae':np.nan,'rmse':np.nan}\n    return {'mae': mean_absolute_error(trues,preds), 'rmse': mean_squared_error(trues,preds,squared=False)}\n\nprint(\"Beginning training: train size\", len(train_data))\nfor epoch in range(1, EPOCHS+1):\n    t0 = time.time()\n    model.train()\n    losses = []\n    for d in train_data:\n        d = d.to(DEVICE)\n        optimizer.zero_grad()\n        out = model(d)\n        loss = criterion(out[d.mask], d.y[d.mask])\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n    valm = eval_list(model, val_data)\n    avg_loss = np.mean(losses) if losses else np.nan\n    print(f\"Epoch {epoch:02d} loss={avg_loss:.4f} val_mae={valm['mae']:.4f} time={(time.time()-t0):.1f}s\")\n    if not np.isnan(valm['mae']) and valm['mae'] < best_val_mae:\n        best_val_mae = valm['mae']; best_state = {k:v.cpu() for k,v in model.state_dict().items()}; patience_ct=0\n        print(\" New best val MAE:\", best_val_mae)\n    else:\n        patience_ct += 1\n        if patience_ct >= patience:\n            print(\"Early stopping triggered.\"); break\n\nif best_state is not None: model.load_state_dict(best_state)\ntorch.save(model.state_dict(), os.path.join(OUTPATH, \"gnn_model_stgcn.pt\"))\nprint(\"Saved model to\", os.path.join(OUTPATH, \"gnn_model_stgcn.pt\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T13:47:06.780080Z","iopub.execute_input":"2025-11-13T13:47:06.780276Z","iopub.status.idle":"2025-11-13T14:45:11.018983Z","shell.execute_reply.started":"2025-11-13T13:47:06.780262Z","shell.execute_reply":"2025-11-13T14:45:11.018242Z"}},"outputs":[{"name":"stdout","text":"Beginning training: train size 49049\nEpoch 01 loss=2521.4459 val_mae=64.7627 time=180.3s\n New best val MAE: 64.76268585821096\nEpoch 02 loss=2084.3268 val_mae=55.3948 time=174.4s\n New best val MAE: 55.39478492775702\nEpoch 03 loss=1991.1981 val_mae=63.1224 time=174.1s\nEpoch 04 loss=1885.2894 val_mae=57.2578 time=173.7s\nEpoch 05 loss=1911.7801 val_mae=52.8507 time=173.3s\n New best val MAE: 52.85072097254214\nEpoch 06 loss=1885.5876 val_mae=54.1477 time=173.5s\nEpoch 07 loss=1898.4580 val_mae=49.7597 time=173.5s\n New best val MAE: 49.75967053340369\nEpoch 08 loss=1903.1978 val_mae=46.6492 time=173.2s\n New best val MAE: 46.64918510164349\nEpoch 09 loss=2047.0667 val_mae=56.4196 time=173.2s\nEpoch 10 loss=1879.1197 val_mae=51.1835 time=174.9s\nEpoch 11 loss=1991.3431 val_mae=60.6739 time=175.3s\nEpoch 12 loss=1862.9183 val_mae=56.0993 time=173.1s\nEpoch 13 loss=1913.5823 val_mae=51.6606 time=174.2s\nEpoch 14 loss=1746.4311 val_mae=52.2579 time=173.2s\nEpoch 15 loss=1787.1309 val_mae=47.7654 time=174.5s\nEpoch 16 loss=1716.3353 val_mae=46.3535 time=173.9s\n New best val MAE: 46.35351978672535\nEpoch 17 loss=1736.6696 val_mae=51.3090 time=173.8s\nEpoch 18 loss=1663.0216 val_mae=47.8097 time=173.9s\nEpoch 19 loss=1661.7328 val_mae=48.7430 time=174.0s\nEpoch 20 loss=1724.7971 val_mae=46.3200 time=174.2s\n New best val MAE: 46.32002051662921\nSaved model to /kaggle/working/gnn_model_stgcn.pt\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# === CELL 13: EVALUATE ON TEST SET & SAVE PREDICTIONS ===\nimport pandas as pd\nrows = []\nmodel.eval()\nwith torch.no_grad():\n    for idx, d in enumerate(test_data):\n        t_from = time_from_list[test_idx[idx]]; t_to = time_to_list[test_idx[idx]]\n        d = d.to(DEVICE)\n        out = model(d).squeeze(1).cpu().numpy()\n        true = d.y.squeeze(1).cpu().numpy()\n        mask = d.mask.cpu().numpy()\n        for node in range(len(zone_list)):\n            if not mask[node]: continue\n            rows.append({\n                \"zone\": zone_list[node],\n                \"time_from\": str(t_from),\n                \"time_to\": str(t_to),\n                \"pred_bookings\": float(out[node]),\n                \"true_bookings\": float(true[node])\n            })\npreds_df = pd.DataFrame(rows)\npreds_path = os.path.join(OUTPATH, \"gnn_predictions_stgcn.csv\")\npreds_df.to_csv(preds_path, index=False)\nprint(\"Saved predictions to\", preds_path)\nif len(preds_df):\n    from sklearn.metrics import mean_absolute_error, mean_squared_error\n    print(\"Test MAE:\", mean_absolute_error(preds_df[\"true_bookings\"], preds_df[\"pred_bookings\"]))\n    print(\"Test RMSE:\", mean_squared_error(preds_df[\"true_bookings\"], preds_df[\"pred_bookings\"], squared=False))\ndisplay(preds_df.head(20))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T14:45:11.021273Z","iopub.execute_input":"2025-11-13T14:45:11.021594Z","iopub.status.idle":"2025-11-13T14:45:32.670007Z","shell.execute_reply.started":"2025-11-13T14:45:11.021575Z","shell.execute_reply":"2025-11-13T14:45:32.669097Z"}},"outputs":[{"name":"stdout","text":"Saved predictions to /kaggle/working/gnn_predictions_stgcn.csv\nTest MAE: 47.5944700421437\nTest RMSE: 76.95262202453314\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"               zone            time_from              time_to  pred_bookings  \\\n0   8986d880d77ffff  2025-08-31 05:15:00  2025-08-31 05:30:00     415.294952   \n1   8986d882c0bffff  2025-08-31 05:15:00  2025-08-31 05:30:00     282.574677   \n2   8986d882e3bffff  2025-08-31 05:15:00  2025-08-31 05:30:00     258.475403   \n3   8986d88412bffff  2025-08-31 05:15:00  2025-08-31 05:30:00     224.293365   \n4   8986d88412fffff  2025-08-31 05:15:00  2025-08-31 05:30:00     172.463684   \n5   8986d88440bffff  2025-08-31 05:15:00  2025-08-31 05:30:00     548.115967   \n6   8986d88449bffff  2025-08-31 05:15:00  2025-08-31 05:30:00     387.760956   \n7   8986d884537ffff  2025-08-31 05:15:00  2025-08-31 05:30:00     142.348190   \n8   8986d88454bffff  2025-08-31 05:15:00  2025-08-31 05:30:00     253.062668   \n9   8986d88454fffff  2025-08-31 05:15:00  2025-08-31 05:30:00     388.059326   \n10  8986d8845c3ffff  2025-08-31 05:15:00  2025-08-31 05:30:00     135.689545   \n11  8986d88460fffff  2025-08-31 05:15:00  2025-08-31 05:30:00     191.402618   \n12  8986d884883ffff  2025-08-31 05:15:00  2025-08-31 05:30:00      45.396431   \n13  8986d884b13ffff  2025-08-31 05:15:00  2025-08-31 05:30:00      53.280746   \n14  8986d884c07ffff  2025-08-31 05:15:00  2025-08-31 05:30:00     334.009766   \n15  8986d884c6fffff  2025-08-31 05:15:00  2025-08-31 05:30:00     214.712677   \n16  8986d884cc7ffff  2025-08-31 05:15:00  2025-08-31 05:30:00     274.669189   \n17  8986d8860b7ffff  2025-08-31 05:15:00  2025-08-31 05:30:00     146.545593   \n18  8986d88626fffff  2025-08-31 05:15:00  2025-08-31 05:30:00     281.729706   \n19  8986d88653bffff  2025-08-31 05:15:00  2025-08-31 05:30:00     201.326355   \n\n    true_bookings  \n0      476.181854  \n1      322.527435  \n2      313.196167  \n3      264.367249  \n4      199.273544  \n5      608.123413  \n6      529.688965  \n7      152.436981  \n8      254.863693  \n9      461.934906  \n10      85.500000  \n11     199.000000  \n12      31.563692  \n13       5.409091  \n14     375.602814  \n15     280.453125  \n16     222.573181  \n17     171.920227  \n18     464.514160  \n19     179.390915  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>zone</th>\n      <th>time_from</th>\n      <th>time_to</th>\n      <th>pred_bookings</th>\n      <th>true_bookings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8986d880d77ffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>415.294952</td>\n      <td>476.181854</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8986d882c0bffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>282.574677</td>\n      <td>322.527435</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8986d882e3bffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>258.475403</td>\n      <td>313.196167</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8986d88412bffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>224.293365</td>\n      <td>264.367249</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8986d88412fffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>172.463684</td>\n      <td>199.273544</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8986d88440bffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>548.115967</td>\n      <td>608.123413</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>8986d88449bffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>387.760956</td>\n      <td>529.688965</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8986d884537ffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>142.348190</td>\n      <td>152.436981</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8986d88454bffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>253.062668</td>\n      <td>254.863693</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8986d88454fffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>388.059326</td>\n      <td>461.934906</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>8986d8845c3ffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>135.689545</td>\n      <td>85.500000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>8986d88460fffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>191.402618</td>\n      <td>199.000000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>8986d884883ffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>45.396431</td>\n      <td>31.563692</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>8986d884b13ffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>53.280746</td>\n      <td>5.409091</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>8986d884c07ffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>334.009766</td>\n      <td>375.602814</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>8986d884c6fffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>214.712677</td>\n      <td>280.453125</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>8986d884cc7ffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>274.669189</td>\n      <td>222.573181</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>8986d8860b7ffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>146.545593</td>\n      <td>171.920227</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>8986d88626fffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>281.729706</td>\n      <td>464.514160</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>8986d88653bffff</td>\n      <td>2025-08-31 05:15:00</td>\n      <td>2025-08-31 05:30:00</td>\n      <td>201.326355</td>\n      <td>179.390915</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# === CELL 14: FINAL DIAGNOSTICS ===\nprint(\"Files saved in\", OUTPATH)\nfor fn in [\"graph_edges.csv\",\"gnn_model_stgcn.pt\",\"gnn_predictions_stgcn.csv\"]:\n    print(\"-\", os.path.join(OUTPATH, fn), \"exists:\", os.path.exists(os.path.join(OUTPATH, fn)))\nprint(\"Zones:\", len(zone_list), \"Features:\", feature_cols, \"L:\", L)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T14:45:32.671041Z","iopub.execute_input":"2025-11-13T14:45:32.671340Z","iopub.status.idle":"2025-11-13T14:45:32.676841Z","shell.execute_reply.started":"2025-11-13T14:45:32.671315Z","shell.execute_reply":"2025-11-13T14:45:32.676067Z"}},"outputs":[{"name":"stdout","text":"Files saved in /kaggle/working/\n- /kaggle/working/graph_edges.csv exists: True\n- /kaggle/working/gnn_model_stgcn.pt exists: True\n- /kaggle/working/gnn_predictions_stgcn.csv exists: True\nZones: 67 Features: ['Completed Trips', 'Traffic Volume', 'Average Speed', 'Congestion Level', 'Temperature', \"Drivers' Earnings\", 'Distance Travelled (km)', 'event importance', 'event type', 'pred_bookings'] L: 4\n","output_type":"stream"}],"execution_count":15}]}